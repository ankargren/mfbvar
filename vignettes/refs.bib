@article{Aguilar2000,
 abstract = {We discuss the development of dynamic factor models for multivariate financial time series, and the incorporation of stochastic volatility components for latent factor processes. Bayesian inference and computation is developed and explored in a study of the dynamic factor structure of daily spot exchange rates for a selection of international currencies. The models are direct generalizations of univariate stochastic volatility models and represent specific varieties of models recently discussed in the growing multivariate stochastic volatility literature. We discuss model fitting based on retrospective data and sequential analysis for forward filtering and short-term forecasting. Analyses are compared with results from the much simpler method of dynamic variance-matrix discounting that, for over a decade, has been a standard approach in applied financial econometrics. We study these models in analysis, forecasting, and sequential portfolio allocation for a selected set of international exchange-rate-return time series. Our goals are to understand a range of modeling questions arising in using these factor models and to explore empirical performance in portfolio construction relative to discount approaches. We report on our experiences and conclude with comments about the practical utility of structured factor models and on future potential model extensions.},
 author = {Omar Aguilar and Mike West},
 journal = {Journal of Business \& Economic Statistics},
 number = {3},
 pages = {338--357},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayesian Dynamic Factor Models and Portfolio Allocation},
 volume = {18},
 year = {2000},
 doi={10.1080/07350015.2000.10524875}
}
  @Manual{Allaire2019,
    title = {RcppParallel: Parallel Programming Tools for 'Rcpp'},
    author = {JJ Allaire and Romain Francois and Kevin Ushey and Gregory Vandenbrouck and Marcus Geelnard and {Intel}},
    year = {2019},
    note = {R package version 4.4.3},
    url = {https://CRAN.R-project.org/package=RcppParallel},
  }
@article{Ankargren2016,
abstract = {{\textcopyright} 2016 The Author(s)This paper first describes financial variables that have been constructed to correspond to various channels in the transmission mechanism. Next, a Bayesian VAR model for the macroeconomy, with priors on the steady states, is augmented with these financial variables and estimated using Swedish data for 1989–2015. The results support three conclusions. First, the financial system is important and the strength of the results is dependent on identification, with the financial variables accounting for 10–25 {\%} of the forecast error variance of Swedish GDP growth. Second, the suggested model produces an earlier signal regarding the probability of recession, compared to a model without financial variables. Third, the model's forecasts for the deep downturn in 2008 and 2009, conditional on the development of the financial variables, outperform a macro-model that lacks financial variables. Furthermore, this improvement in modelling Swedish GDP growth during the financial crisis does not come at the expense of unconditional predictive power. Taken together, the results suggest that the proposed model presents an accessible possibility to analyse the macro-financial linkages and the GDP developments, especially during a financial crisis.},
author = {Ankargren, Sebastian and Bjellerup, M{\aa}rten and Shahnazarian, Hovick},
doi = {10.1007/s00181-016-1175-4},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Ankargren, Bjellerup, Shahnazarian{\_}2016.pdf:pdf},
journal = {Empirical Economics},
keywords = {Bayesian VAR,Business cycle,Credit cycle,Financial indicators,Macroeconomy,Transmission channels},
pages = {1553--1586},
publisher = {Springer Berlin Heidelberg},
title = {{The Importance of the Financial System for the Real Economy}},
year = {2017},
volume = {53},
number = {4}
}
@techreport{Ankargren2018,
author = {Ankargren, Sebastian and Unosson, M{\aa}ns and Yang, Yukai},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Ankargren, Yang, Unosson{\_}2017.pdf:pdf},
keywords = {forecast-,macroeconometrics,marginal data density,state space models,var},
pages = {1--30},
title = {{A Mixed-Frequency Bayesian Vector Autoregression with a Steady-State Prior}},
year = {2018},
institution = {Department of Statistics, Uppsala University},
type = {Working Paper No.},
number = {2018:2}
}
@ARTICLE{AnkargrenUnossonYang2019,
author = {Ankargren, Sebastian and Unosson, M\r{a}ns and Yang, Yukai},
file = {:Users/sebastianankargren/Documents/Mendeley Desktop/Ankargren, Unosson, Yang - 2018 - A mixed-frequency Bayesian vector autoregression with a steady-state prior.pdf:pdf},
title = {A Flexible Mixed-Frequency Vector Autoregression with a Steady-State Prior},
year = {2019},
arxivId = {arXiv:1911.09151},
note = {\href{http://arxiv.org/abs/1911.09151}{arXiv:1911.09151}},
journal = {Preprint available on arXiv}
}

@ARTICLE{Ankargren2019,
author = {Ankargren, Sebastian and Jon\'{e}us, Paulina},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Ankargren, Yang, Unosson{\_}2017.pdf:pdf},
keywords = {forecast-,macroeconometrics,marginal data density,state space models,var},
title = {\GG{1}Simulation smoothing for large mixed-frequency VARs},
year = {2019},
arxivId = {arXiv:1907.01075},
note = {\href{http://arxiv.org/abs/1907.01075}{arXiv:1907.01075}},
journal = {Preprint available on arXiv}
}
@ARTICLE{Ankargren2019b,
author = {Ankargren, Sebastian and Jon\'{e}us, Paulina},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Ankargren, Yang, Unosson{\_}2017.pdf:pdf},
keywords = {forecast-,macroeconometrics,marginal data density,state space models,var},
title = {\GG{2}Estimating large mixed-frequency Bayesian VAR Models},
year = {2019},
arxivId = {arXiv:1912.02231},
note = {\href{http://arxiv.org/abs/1912.02231}{arXiv:1912.02231}},
journal = {Preprint available on arXiv}
}

@article{Baffigi2004,
abstract = {Quantitative information on the current state of the economy is crucial to economic policy-making and to early understanding of the economic situation, but the quarterly national account (NA) data for GDP in the euro area are released with a substantial delay. The aim of the paper is to examine the forecast ability of bridge models (BM) for GDP growth in the euro area. BM 'bridge the gap' between the information content of timely updated indicators and the delayed (but more complete) NA. In this paper, BM are estimated for aggregate GDP and components both area-wide and for the three main countries of the euro area. Their short-term (one- and two-quarter ahead) forecasting performance is assessed with respect to benchmark univariate/multivariate statistical models, and a small structural model. The paper shows that national BM fare better than benchmark models. In addition, euro area GDP and its components are more precisely predicted by aggregating national forecasts. {\textcopyright} 2003 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.},
author = {Baffigi, Alberto and Golinelli, Roberto and Parigi, Giuseppe},
doi = {10.1016/S0169-2070(03)00067-0},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Baffigi, Golinelli, Parigi{\_}2004.pdf:pdf},
isbn = {1390512092600},
journal = {International Journal of Forecasting},
keywords = {Bridge model,Out-of-sample forecasting accuracy,Short-term GDP forecast for the euro area},
number = {3},
pages = {447--460},
title = {{Bridge models to forecast the euro area GDP}},
volume = {20},
year = {2004}
}
@article{Banbura2010,
abstract = {This paper shows that vector auto regression (VAR) with Bayesian shrinkage is an appropriate tool for large dynamic models. We build on the results of De Mol and co-workers (2008) and show that, when the degree of shrinkage is set in relation to the cross-sectional dimension, the forecasting performance of small monetary VARs can be improved by adding additional macroeconomic variables and sectoral information. In addition, we show that large VARs with shrinkage produce credible impulse responses and are suitable for structural analysis.},
author = {Ba{\'{n}}bura, Marta and Giannone, Domenico and Reichlin, Lucrezia},
doi = {10.1002/jae.1137},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Ba{\'{n}}bura, Giannone, Reichlin{\_}2010.pdf:pdf},
journal = {Journal of Applied Econometrics},
number = {1},
pages = {71--92},
title = {{Large Bayesian Vector Auto Regressions}},
volume = {25},
year = {2010}
}
@article{Bhattacharya2016,
abstract = {SUMMARY We propose an efficient way to sample from a class of structured multivariate Gaussian distributions. The proposed algorithm only requires matrix multiplications and linear system solutions. Its computational complexity grows linearly with the dimension, unlike existing algorithms that rely on Cholesky factoriza-tions with cubic complexity. The algorithm is broadly applicable in settings where Gaussian scale mixture priors are used on high-dimensional parameters. Its effectiveness is illustrated through a high-dimensional regression problem with a horseshoe prior on the regression coefficients. Other potential applications are outlined.},
archivePrefix = {arXiv},
arxivId = {1506.04778},
author = {Bhattacharya, Anirban and Chakraborty, Antik and Mallick, Bani K.},
doi = {10.1093/biomet/asw042},
file = {:Users/sebastianankargren/Documents/Mendeley Desktop/Bhattacharya, Chakraborty, Mallick - 2016 - Fast sampling with Gaussian scale mixture priors in high-dimensional regression.pdf:pdf},
journal = {Biometrika},
keywords = {Confidence interval,Gaussian scale mixture,Global-local prior,Shrinkage,Sparsity.,bayesian,high dimensional,mcmc},
mendeley-tags = {bayesian,high dimensional,mcmc},
number = {4},
pages = {985--991},
title = {{Fast sampling with Gaussian scale mixture priors in high-dimensional regression}},
volume = {103},
year = {2016}
}
@article{Carriero2015,
abstract = {This paper develops a method for producing current-quarter forecasts of GDP growth with a (possibly large) range of available within-the-quarter monthly observations of economic indicators, such as employment and industrial production, and financial indicators, such as stock prices and interest rates. In light of existing evidence of time variation in the variances of shocks to GDP, we consider versions of the model with both constant variances and stochastic volatility. We also evaluate models with either constant or time-varying regression coefficients. We use Bayesian methods to estimate the model, in order to facilitate providing shrinkage on the (possibly large) set of model parameters and conveniently generate predictive densities. We provide results on the accuracy of nowcasts of real-time GDP growth in the U.S. from 1985 through 2011. In terms of point forecasts, our proposal is comparable to alternative econometric methods and survey forecasts. In addition, it provides reliable density forecasts, for which the stochastic volatility specification is quite useful, while parameter time-variation does not seem to matter.},
author = {Carriero, Andrea and Clark, Todd E. and Marcellino, Massimiliano},
doi = {10.1111/rssa.12092},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Carriero, Clark, Marcellino{\_}2015(2).pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series A: Statistics in Society},
keywords = {Bayesian methods,Forecasting,Mixed frequency models,Prediction},
number = {4},
pages = {837--862},
title = {{Realtime Nowcasting with a Bayesian Mixed Frequency Model with Stochastic Volatility}},
volume = {178},
year = {2015}
}
@article{Carriero2016,
author = {Carriero, Andrea and Clark, Todd E. and Marcellino, Massimiliano},
doi = {10.1080/07350015.2015.1040116},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Carriero, Clark, Marcellino - 2016 - Common Drifting Volatility in Large Bayesian VARs.pdf:pdf},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {bayesian,forecasting,high dimensional,mcmc,prediction,stochastic volatility,var},
mendeley-tags = {bayesian,high dimensional,mcmc,stochastic volatility,var},
number = {3},
pages = {375--390},
title = {Common Drifting Volatility in Large {Bayesian} {VARs}},
volume = {34},
year = {2016}
}
@article{Chib1995,
abstract = {In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution. Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of resulting estimate is derived. The ideas are applied to probit regression and finite mixture models.},
author = {Chib, Siddhartha},
doi = {10.1080/01621459.1995.10476635},
file = {:C$\backslash$:/Users/seban876/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chib - 1995 - Marginal Likelihood from the Gibbs Output(2).pdf:pdf},
isbn = {0162-1459},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Estimation of normalizing constant,Finite mixture models,Linear regression,Markov chain Monte Carlo,Markov mixture model,Multivariate density estimation,Numerical standard error,Probit regression,Reduced conditional density},
number = {432},
pages = {1313--1321},
pmid = {12510683},
title = {Marginal Likelihood from the {Gibbs} Output},
volume = {90},
year = {1995}
}
@article{Clark2011,
abstract = {Central banks and other forecasters are increasingly interested in various aspects of density forecasts. However, recent sharp changes in macroeconomic volatility, including the Great Moderation and the more recent sharp rise in volatility associated with increased variation in energy prices and the deep global recession?pose significant challenges to density forecasting. Accordingly, this paper examines, with real-time data, density forecasts of U.S. GDP growth, unemployment, inflation, and the federal funds rate from Bayesian vector autoregression (BVAR) models with stochastic volatility. The results indicate that adding stochastic volatility to BVARs materially improves the real-time accuracy of density forecasts. This article has supplementary material online.$\backslash$nCentral banks and other forecasters are increasingly interested in various aspects of density forecasts. However, recent sharp changes in macroeconomic volatility, including the Great Moderation and the more recent sharp rise in volatility associated with increased variation in energy prices and the deep global recession?pose significant challenges to density forecasting. Accordingly, this paper examines, with real-time data, density forecasts of U.S. GDP growth, unemployment, inflation, and the federal funds rate from Bayesian vector autoregression (BVAR) models with stochastic volatility. The results indicate that adding stochastic volatility to BVARs materially improves the real-time accuracy of density forecasts. This article has supplementary material online.},
author = {Clark, Todd E.},
doi = {10.1198/jbes.2010.09248},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Clark{\_}2011.pdf:pdf},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {bayesian methods,steady-state prior},
number = {3},
pages = {327--341},
title = {{Real-Time Density Forecasts From Bayesian Vector Autoregressions With Stochastic Volatility}},
volume = {29},
year = {2011}
}
@article{Clark2015,
author = {Clark, Todd E. and Ravazzolo, Francesco},
title = {Macroeconomic Forecasting Performance under Alternative Specifications of Time-Varying Volatility},
journal = {Journal of Applied Econometrics},
volume = {30},
number = {4},
pages = {551-575},
doi = {10.1002/jae.2379},
abstract = {SummaryThis paper compares alternative models of time-varying volatility on the basis of the accuracy of real-time point and density forecasts of key macroeconomic time series for the USA. We consider Bayesian autoregressive and vector autoregressive models that incorporate some form of time-varying volatility, precisely random walk stochastic volatility, stochastic volatility following a stationary AR process, stochastic volatility coupled with fat tails, GARCH and mixture of innovation models. The results show that the AR and VAR specifications with conventional stochastic volatility dominate other volatility specifications, in terms of point forecasting to some degree and density forecasting to a greater degree. Copyright © 2014 John Wiley \& Sons, Ltd.},
year = {2015}
}


@article{Cogley2005,
title = "Drifts and Volatilities: {Monetary} Policies and Outcomes in the Post {WWII} {US}",
journal = "Review of Economic Dynamics",
volume = "8",
number = "2",
pages = "262--302",
year = "2005",
doi = "10.1016/j.red.2004.10.009",
author = "Timothy Cogley and Thomas J. Sargent",
abstract = "For a VAR with drifting coefficients and stochastic volatilities, we present posterior densities for several objects that are pertinent for designing and evaluating monetary policy. These include measures of inflation persistence, the natural rate of unemployment, a core rate of inflation, and ‘activism coefficients’ for monetary policy rules. Our posteriors imply substantial variation of all of these objects for post WWII US data. After adjusting for changes in volatility, persistence of inflation increases during the 1970s, then falls in the 1980s and 1990s. Innovation variances change systematically, being substantially larger in the late 1970s than during other times. Measures of uncertainty about core inflation and the degree of persistence covary positively. We use our posterior distributions to evaluate the power of several tests that have been used to test the null hypothesis of time-invariance of autoregressive coefficients of VARs against the alternative of time-varying coefficients. Except for one, we find that those tests have low power against the form of time variation captured by our model."
}
@article{DAgostino2013,
author = {D'Agostino, Antonello and Gambetti, Luca and Giannone, Domenico},
title = {Macroeconomic Forecasting and Structural Change},
journal = {Journal of Applied Econometrics},
volume = {28},
number = {1},
pages = {82--101},
doi = {10.1002/jae.1257},
abstract = {SUMMARY The aim of this paper is to assess whether modeling structural change can help improving the accuracy of macroeconomic forecasts. We conduct a simulated real-time out-of-sample exercise using a time-varying coefficients vector autoregression (VAR) with stochastic volatility to predict the inflation rate, unemployment rate and interest rate in the USA. The model generates accurate predictions for the three variables. In particular, the forecasts of inflation are much more accurate than those obtained with any other competing model, including fixed coefficients VARs, time-varying autoregressions and the naïve random walk model. The results hold true also after the mid 1980s, a period in which forecasting inflation was particularly hard. Copyright © 2011 John Wiley \& Sons, Ltd.},
year = {2013}
}
@incollection{DelNegro2011,
address = {Oxford},
author = {{Del Negro}, Marco and Schorfheide, Frank},
booktitle = {The Oxford Handbook of Bayesian Econometrics},
doi = {10.1093/oxfordhb/9780199559084.013.0008},
editor = {Geweke, John and Koop, Gary and van Dijk, Herman},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Del Negro, Schorfheide{\_}2011.pdf:pdf},
keywords = {Autocovariance properties,Bayesian methods,Macroeconomists,Vector autoregressive models},
pages = {293--389},
publisher = {Oxford University Press},
title = {{Bayesian Macroeconometrics}},
year = {2011}
}
@techreport{Dieppe2016,
author = {Dieppe, Alistair and Legrand, Romain and van Roye, Bj{\"{o}}rn},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Dieppe, Legrand, van Roye{\_}2016.pdf:pdf},
institution = {European Central Bank},
keywords = {Bayesian VAR,VAR,econometric software,forecasting,panel Bayesian VAR,structural},
type = {Working Paper No.},
title = {{The BEAR Toolbox}},
year = {2016},
number= {1934}
}
@article{Durbin2002,
author = {Durbin, J and Koopman, S J},
doi = {10.1093/biomet/89.3.603},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Durbin, Koopman{\_}2002.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {603--615},
title = {{A Simple and Efficient Simulation Smoother for State Space Time Series Analysis}},
volume = {89},
year = {2002}
}
@book{Durbin2012,
address = {Oxford, UK},
author = {Durbin, James and Koopman, Siem Jan},
doi = {10.1093/acprof:oso/9780199641178.001.0001},
edition = {Second},
publisher = {Oxford University Press},
title = {{Time Series Analysis by State Space Methods}},
year = {2012}
}
@article{Eddelbuettel2011,
author = {Eddelbuettel, Dirk and Francois, Romain},
doi = {10.18637/jss.v040.i08},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Eddelbuettel, Francois{\_}2011.pdf:pdf},
isbn = {9781461468677},
journal = {Journal of Statistical Software},
keywords = {c,call,foreign function interface,r},
number = {8},
title = {{Seamless R and C++ Intgration with Rcpp}},
volume = {40},
year = {2011}
}
@article{Eddelbuettel2014,
abstract = {The R statistical environment and language has demonstrated particular strengths for interactive development of statistical algorithms, as well as data modelling and visualisation. Its current implementation has an interpreter at its core which may result in a performance penalty in comparison to directly executing user algorithms in the native machine code of the host CPU. In contrast, the C++ language has no built-in visualisation capabilities, handling of linear algebra or even basic statistical algorithms; however, user programs are converted to high-performance machine code, ahead of execution. A new method avoids possible speed penalties in R by using the Rcpp extension package in conjunction with the Armadillo C++ matrix library. In addition to the inherent performance advantages of compiled code, Armadillo provides an easy-to-use template-based meta-programming framework, allowing the automatic pooling of several linear algebra operations into one, which in turn can lead to further speedups. With the aid of Rcpp and Armadillo, conversion of linear algebra centred algorithms from R to C++ becomes straightforward. The algorithms retain the overall structure as well as readability, all while maintaining a bidirectional link with the host R environment. Empirical timing comparisons of R and C++ implementations of a Kalman filtering algorithm indicate a speedup of several orders of magnitude. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {Eddelbuettel, Dirk and Sanderson, Conrad},
doi = {10.1016/j.csda.2013.02.005},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Eddelbuettel, Sanderson{\_}2014.pdf:pdf},
journal = {Computational Statistics and Data Analysis},
keywords = {C++,Linear algebra,R,Software},
pages = {1054--1063},
publisher = {Elsevier B.V.},
title = {{RcppArmadillo: Accelerating R with High-Performance C++ Linear Algebra}},
volume = {71},
year = {2014}
}
@article{Foroni2013,
abstract = {The development of models for variables sampled at di¤erent frequencies has attracted substantial interest in the recent econometric literature. In this paper we provide an overview of the most common techniques, including bridge equa- tions, MIxed DAta Sampling (MIDAS) models, mixed frequency VARs, and mixed frequency factor models. We also consider alternative techniques for handling the ragged edge of the data, due to asynchronous publication. Finally, we survey the main empirical applications based on alternative mixed frequency models. J.E.L.},
author = {Foroni, Claudia and Marcellino, Massimiliano},
doi = {10.2139/ssrn.2268912},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Foroni, Marcellino{\_}2013.pdf:pdf},
keywords = {Claudia Foroni,Massimiliano Marcellino},
title = {{A Survey of Econometric Methods for Mixed-Frequency Data}},
year = {2013}
}

@article{FuentesAlbero2013,
abstract = {We introduce two estimators for estimating the Marginal Data Density (MDD) from the Gibbs output. Our methods are based on exploiting the analytical tractability condition, which requires that some parameter blocks can be analytically integrated out from the conditional posterior densities. This condition is satisfied by several widely used time series models. An empirical application to six-variate VAR models shows that the bias of a fully computational estimator is sufficiently large to distort the implied model rankings. One of the estimators is fast enough to make multiple computations of MDDs in densely parameterized models feasible. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Fuentes-Albero, Cristina and Melosi, Leonardo},
doi = {10.1016/j.jeconom.2013.03.002},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Fuentes-Albero, Melosi{\_}2013.pdf:pdf},
journal = {Journal of Econometrics},
keywords = {Bayesian econometrics,Gibbs sampler,Marginal likelihood,Reciprocal importance sampling,Time series econometrics},
number = {2},
pages = {132--141},
publisher = {Elsevier B.V.},
title = {{Methods for Computing Marginal Data Densities from the Gibbs Output}},
volume = {175},
year = {2013}
}
@article{Gelfand1992,
author = {Gelfand, Alan E and Smith, Adrian F M and Lee, Tai-ming},
doi = {10.2307/2290286},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Gelfand, Smith, Lee{\_}1992.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {actuarial graduation,bayesian inference,constrained parameter problems arise,constrained parameters,dinal categorical data,gibbs sampler,in a wide variety,including bioassay,of applications,or-,reliability devel-,response surfaces,truncated data},
number = {418},
pages = {523--532},
title = {{Bayesian Analysis of Constrained Parameter and Truncated Data Problems Using Gibbs Sampling}},
volume = {87},
year = {1992}
}
@article{Ghysels2016b,
author = {Ghysels, Eric and Kvedaras, Virmantas and Zemlys, Vaidotas},
doi = {10.18637/jss.v072.i04},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Ghysels, Kvedaras, Zemlys{\_}2016.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {midas,specification test},
number = {4},
title = {{Mixed Frequency Data Sampling Regression Models: The R Package midasr}},
volume = {72},
year = {2016}
}
@article{Ghysels2007,
abstract = {We explore mixed data sampling (henceforth MIDAS) regression models. The regressions involve time series data sampled at different frequencies. Volatility and related processes are our prime focus, though the regression method has wider applications in macroeconomics and finance, among other areas. The regressions combine recent developments regarding estimation of volatility and a not-so-recent literature on distributed lag models. We study various lag structures to parameterize parsimoniously the regressions and relate them to existing models. We also propose several new extensions of the MIDAS framework. The paper concludes with an empirical section where we provide further evidence and new results on the risk–return trade-off. We also report empirical evidence on microstructure noise and volatility forecasting.},
author = {Ghysels, Eric and Sinko, Arthur and Valkanov, Rossen},
doi = {10.1080/07474930600972467},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Ghysels, Sinko, Valkanov{\_}2007.pdf:pdf},
journal = {Econometric Reviews},
keywords = {C22,C53,Microstructure noise,Nonlinear MIDAS,Risk,Tick-by-tick applications,Volatility},
number = {1},
pages = {53--90},
title = {{MIDAS Regressions: Further Results and New Directions}},
volume = {26},
year = {2007}
}
@techreport{Gotz2018,
author = {G{\"{o}}tz, Thomas B. and Hauzenberger, Klemens},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/G{\"{o}}tz, Hauzenberger - 2018 - Large mixed-frequency VARs with a parsimonious time-varying parameter structure.pdf:pdf},
institution = {Deutsche Bundesbank},
keywords = {Bayesian VAR,Common Stochastic,Forecasting,Mixed Frequencies,Time-Varying Intercepts,Volatility},
number = {40},
type = {Discussion Paper No.},
title = {Large Mixed-Frequency {VARs} with a Parsimonious Time-Varying Parameter Structure},
year = {2018}
}
@article{Griffin2010,
abstract = {This paper considers the effects of placing an absolutely continuous prior distribution on the regression coefficients of a linear model. We show that the posterior expectation is a matrix-shrunken version of the least squares estimate where the shrinkage matrix depends on the derivatives of the prior predictive density of the least squares estimate. The special case of the normal-gamma prior, which generalizes the Bayesian Lasso (Park and Casella 2008), is studied in depth. We discuss the prior interpretation and the posterior effects of hyperparameter choice and suggest a data-dependent default prior. Simulations and a chemometric example are used to compare the performance of the normal-gamma and the Bayesian Lasso in terms of out-of-sample predictive performance.},
author = {Griffin, Jim E. and Brown, Philip J.},
doi = {10.1214/10-BA507},
file = {:C$\backslash$:/Users/seban876/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffin, Brown - 2010 - Inference with normal-gamma prior distributions in regression problems.pdf:pdf},
isbn = {1936-0975},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {"Spike-and-slab" prior,Bayesian lasso,Markov chain monte carlo,Multiple regression,Normal-gamma prior,Posterior moments,Scale mixture of normals,Shrinkage,p ≥ n},
number = {1},
pages = {171--188},
title = {Inference with Normal-Gamma Prior Distributions in Regression Problems},
volume = {5},
year = {2010}
}
@book{Hamilton1994,
address = {Princeton, NJ},
author = {Hamilton, James D.},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Hamilton{\_}1994.pdf:pdf},
publisher = {Princeton University Press},
title = {{Time Series Analysis}},
year = {1994}
}

@Article{Hormann2014,
author="H{\"o}rmann, Wolfgang
and Leydold, Josef",
title="Generating generalized inverse Gaussian random variates",
journal="Statistics and Computing",
year="2014",
month="Jul",
day="01",
volume="24",
number="4",
pages="547--557",
abstract="The generalized inverse Gaussian distribution has become quite popular in financial engineering. The most popular random variate generator is due to Dagpunar (Commun. Stat., Simul. Comput. 18:703--710, 1989). It is an acceptance-rejection algorithm method based on the Ratio-of-Uniforms method. However, it is not uniformly fast as it has a prohibitive large rejection constant when the distribution is close to the gamma distribution. Recently some papers have discussed universal methods that are suitable for this distribution. However, these methods require an expensive setup and are therefore not suitable for the varying parameter case which occurs in, e.g., Gibbs sampling. In this paper we analyze the performance of Dagpunar's algorithm and combine it with a new rejection method which ensures a uniformly fast generator. As its setup is rather short it is in particular suitable for the varying parameter case.",
issn="1573-1375",
doi="10.1007/s11222-013-9387-3",
url="https://doi.org/10.1007/s11222-013-9387-3"
}


@article{Huber2019,
author = {Florian Huber and Martin Feldkircher},
title = {Adaptive Shrinkage in {Bayesian} Vector Autoregressive Models},
journal = {Journal of Business \& Economic Statistics},
volume = {37},
number = {1},
pages = {27--39},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/07350015.2016.1256217}
}
@article{Jarocinski2015,
abstract = {The correct implementation of the Durbin and Koopman simulation smoother is explained. A possible misunderstanding is pointed out and clarified for both the basic state space model with a non-zero mean of the initial state and with time-varying intercepts (mean adjustments).},
author = {Jaroci{\'{n}}ski, Marek},
doi = {10.1016/j.csda.2015.05.001},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Jaroci{\'{n}}ski{\_}2015.pdf:pdf},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {Simulation smoother,State space model,Trend output},
pages = {1--3},
publisher = {Elsevier B.V.},
title = {{A Note on Implementing the Durbin and Koopman Simulation Smoother}},
volume = {91},
year = {2015}
}
@techreport{Jarocinski2008,
abstract = {This paper estimates a Bayesian VAR for the US economy which includes a housing sector and addresses the following questions. Can developments in the housing sector be explained on the basis of developments in real and nominal GDP and interest rates? What are the effects of housing demand shocks on the economy? How does monetary policy affect the housing market? What are the implications of house price developments for the stance of monetary policy? Regarding the latter question, we implement a version of a Monetary Conditions Index (MCI) due to C{\'{e}}spedes et al. (2006).},
author = {Jaroci{\'{n}}ski, Marek and Smets, Frank},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Jaroci{\'{n}}ski, Smets{\_}2008.pdf:pdf},
type = {Working Paper No.},
institution = {European Central Bank},
keywords = {Bayesian VAR,JEL Classification: E3-E4,Keywords: House prices,conditional forecast.,monetary conditions index,monetary policy shock},
title = {{House Prices and the Stance of Monetary Policy}},
number = {891},
year = {2008}
}
@book{Karlsson2013,
abstract = {This chapter reviews Bayesian methods for inference and forecasting with VAR models. Bayesian inference and, by extension, forecasting depends on numerical methods for simulating from the posterior distribution of the parameters and special attention is given to the implementation of the simulation algorithm. ?? 2013 Elsevier B.V.},
author = {Karlsson, Sune},
booktitle = {Handbook of Economic Forecasting},
doi = {10.1016/B978-0-444-62731-5.00015-4},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Karlsson{\_}2013.pdf:pdf},
keywords = {Cointegration,Conditional forecasts,Large VAR,Markov chain Monte Carlo,Model selection,Stochastic volatility,Structural VAR,Time-varying parameters},
pages = {791--897},
publisher = {Elsevier B.V.},
title = {{Forecasting with Bayesian Vector Autoregression}},
volume = {2},
year = {2013}
}

@article{Kastner2014,
abstract = {Bayesian inference for stochastic volatility models using MCMC methods highly depends on actual parameter values in terms of sampling efficiency. While draws from the posterior utilizing the standard centered parameterization break down when the volatility of volatility parameter in the latent state equation is small, non-centered versions of the model show deficiencies for highly persistent latent variable series. The novel approach of ancillarity-sufficiency interweaving has recently been shown to aid in overcoming these issues for a broad class of multilevel models. It is demonstrated how such an interweaving strategy can be applied to stochastic volatility models in order to greatly improve sampling efficiency for all parameters and throughout the entire parameter range. Moreover, this method of "combining best of different worlds" allows for inference for parameter constellations that have previously been infeasible to estimate without the need to select a particular parameterization beforehand. {\textcopyright} 2014 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1706.05280},
author = {Kastner, Gregor and Fr{\"{u}}hwirth-Schnatter, Sylvia},
doi = {10.1016/j.csda.2013.01.002},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Kastner, Fr{\"{u}}hwirth-Schnatter - 2014 - Ancillarity-sufficiency interweaving strategy (ASIS) for boosting MCMC estimation of stochastic vo.pdf:pdf},
journal = {Computational Statistics and Data Analysis},
keywords = {Auxiliary mixture sampling,Exchange rate data,Markov chain Monte Carlo,Massively parallel computing,Non-centering,State space model},
pages = {408--423},
publisher = {Elsevier B.V.},
title = {Ancillarity-Sufficiency Interweaving Strategy ({ASIS}) for Boosting {MCMC} Estimation of Stochastic Volatility Models},
volume = {76},
year = {2014}
}


@article{Kastner2016,
abstract = {The R package stochvol provides a fully Bayesian implementation of heteroskedasticity modeling within the framework of stochastic volatility. It utilizes Markov chain Monte Carlo (MCMC) samplers to conduct inference by obtaining draws from the posterior distribution of parameters and latent variables which can then be used for predicting future volatilities. The package can straightforwardly be employed as a stand-alone tool; moreover, it allows for easy incorporation into other MCMC samplers. The main focus of this paper is to show the functionality of stochvol. In addition, it provides a brief mathematical description of the model, an overview of the sampling schemes used, and several illustrative examples using exchange rate data.},
author = {Kastner, Gregor},
doi = {10.18637/jss.v069.i05},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Kastner - 2016 - Dealing with Stochastic Volatility in Time Series Using the iRi Package bstochvolb.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {ancillarity-sufficiency interweaving strategy,asis,auxiliary mixture sam-,bayesian inference,financial time series,heteroskedas-,markov chain monte carlo,mcmc,pling,state-space model,ticity},
number = {5},
title = {Dealing with Stochastic Volatility in Time Series Using the \texttt{R} Package \texttt{stochvol}},
volume = {69},
year = {2016}
}

@article{Kastner2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.08154v3},
author = {Kastner, Gregor and Fr{\"{u}}hwirth-Schnatter, Sylvia and {Freitas Lopes}, Hedibert},
doi = {10.1080/10618600.2017.1322091},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Kastner, Fr{\"{u}}hwirth-Schnatter, Freitas Lopes - 2017 - Efficient Bayesian Inference for Multivariate Factor Stochastic Volatility Model(2).pdf:pdf;:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Kastner, Fr{\"{u}}hwirth-Schnatter, Freitas Lopes - 2017 - Efficient Bayesian Inference for Multivariate Factor Stochastic Volatility Models.pdf:pdf},
journal = {Journal of Computational and Graphical Statistics},
keywords = {bayesian,mcmc,stochastic volatility,time series},
mendeley-tags = {bayesian,mcmc,stochastic volatility,time series},
number = {4},
pages = {905--917},
title = {Efficient {Bayesian} Inference for Multivariate Factor Stochastic Volatility Models},
volume = {26},
year = {2017}
}
@ARTICLE{KastnerHuber2018,
       author = {{Kastner}, Gregor and {Huber}, Florian},
        title = {Sparse {Bayesian} Vector Autoregressions in Huge Dimensions},
     keywords = {Statistics - Computation, Economics - Econometrics, Statistics - Applications, Statistics - Methodology},
         year = "2018",
       arxiv = {1704.03239},
 primaryClass = {stat.CO},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System},
arxivId = {arXiv:1704.0323},
note = {\href{http://arxiv.org/abs/1704.03239}{arXiv:1704.03239}},
journal = {Preprint available on arXiv}
}

@Manual{Kastner2018,
  title = {{factorstochvol}: {B}ayesian Estimation of (Sparse) Latent Factor Stochastic Volatility Models},
  author = {Gregor Kastner},
  year = {2019},
  note = {R package version 0.9},
  url = {https://cran.r-project.org/package=factorstochvol},
}

@Manual{Kastner2019,
  title = {{stochvol}: Efficient Bayesian Inference for Stochastic Volatility ({SV}) Models},
  author = {Gregor Kastner and Darjus Hosszejni},
  year = {2019},
  note = {R package version 2.0.4},
  url = {https://CRAN.R-project.org/package=stochvol},
}

@Article{Hosszejni2019,
  title = {Modeling Univariate and Multivariate Stochastic Volatility in {R} with {stochvol} and {factorstochvol}},
  author = {Darjus Hosszejni and Gregor Kastner},
  journal = {R package vignette},
  year = {2019},
  url = {https://CRAN.R-project.org/package=factorstochvol/vignettes/paper.pdf},
}

@Article{Zeileis2005,
  title = {zoo: S3 Infrastructure for Regular and Irregular Time Series},
  author = {Achim Zeileis and Gabor Grothendieck},
  journal = {Journal of Statistical Software},
  year = {2005},
  volume = {14},
  number = {6},
  pages = {1--27},
  doi = {10.18637/jss.v014.i06},
}

@article{Kim1998,
    author = {Kim, Sangjoon and Shephard, Neil and Chib, Siddhartha},
    title = {Stochastic Volatility: {Likelihood} Inference and Comparison with {ARCH} Models},
    journal = {The Review of Economic Studies},
    volume = {65},
    number = {3},
    pages = {361--393},
    year = {1998},
    month = {07},
    abstract = "{In this paper, Markov chain Monte Carlo sampling methods are exploited to provide a unified, practical likelihood-based framework for the analysis of stochastic volatility models. A highly effective method is developed that samples all the unobserved volatilities at once using an approximating offset mixture model, followed by an importance reweighting procedure. This approach is compared with several alternative methods using real data. The paper also develops simulation-based methods for filtering, likelihood evaluation and model failure diagnostics. The issue of model choice using non-nested likelihood ratios and Bayes factors is also investigated. These methods are used to compare the fit of stochastic volatility and GARCH models. All the procedures are illustrated in detail.}",
    issn = {0034-6527},
    doi = {10.1111/1467-937X.00050},
}


  @Manual{Kleen2018,
    title = {alfred: Downloading Time Series from ALFRED Database for Various
Vintages},
    author = {Onno Kleen},
    year = {2018},
    note = {R package version 0.1.6},
    url = {https://CRAN.R-project.org/package=alfred},
  }

@article{Litterman1986,
author = {Litterman, Robert B},
doi = {10.1080/07350015.1986.10509485},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Litterman{\_}1986.pdf:pdf},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {appears to generate fore-,bayesian,developed that,for the first time,has been disenchanted with,in recent years a,models,not,outlook,statistical time series model,the large structural econometric,then commonly in use,this disenchantment was based,time series},
number = {1},
pages = {1--4},
title = {{A Statistical Approach to Economic Forecasting}},
volume = {4},
year = {1986}
}

@article{Louzis2019,
author = {Louzis, Dimitrios P.},
title = {Steady-state modeling and macroeconomic forecasting quality},
journal = {Journal of Applied Econometrics},
volume = {34},
number = {2},
pages = {285--314},
doi = {10.1002/jae.2657},
abstract = {Summary Vector autoregressions (VARs) with informative steady-state priors are standard forecasting tools in empirical macroeconomics. This study proposes (i) an adaptive hierarchical normal-gamma prior on steady states, (ii) a time-varying steady-state specification which accounts for structural breaks in the unconditional mean, and (iii) a generalization of steady-state VARs with fat-tailed and heteroskedastic error terms. Empirical analysis, based on a real-time dataset of 14 macroeconomic variables, shows that, overall, the hierarchical steady-state specifications materially improve out-of-sample forecasting for forecasting horizons longer than 1 year, while the time-varying specifications generate superior forecasts for variables with significant changes in their unconditional mean.},
year = {2019}
}
@article{Mariano2003,
abstract = {Popular monthly coincident indices of business cycles, e.g. the composite index and the Stock–Watson coincident index, have two shortcomings. First, they ignore information contained in quarterly indicators such as real GDP. Second, they lack economic interpretation; hence the heights of peaks and the depths of troughs depend on the choice of an index. This paper extends the Stock–Watson coincident index by applying maximum likelihood factor analysis to a mixed-frequency series of quarterly real GDP and monthly coincident business cycle indicators. The resulting index is related to latent monthly real GDP. Copyright {\textcopyright} 2002 John Wiley {\&} Sons, Ltd.},
author = {Mariano, Roberto S. and Murasawa, Yasutomo},
doi = {10.1002/jae.695},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Mariano, Murasawa{\_}2003.pdf:pdf},
issn = {08837252},
journal = {Journal of Applied Econometrics},
number = {4},
pages = {427--443},
title = {A New Coincident Index of Business Cycles Based on Monthly and Quarterly Series},
volume = {18},
year = {2003}
}
@article{McCausland2011,
title = "Simulation smoothing for state-space models: A computational efficiency analysis",
journal = "Computational Statistics \& Data Analysis",
volume = "55",
number = "1",
pages = "199--212",
year = "2011",
doi = "10.1016/j.csda.2010.07.009",
author = "William J. McCausland and Shirley Miller and Denis Pelletier",
keywords = "State–space models, Markov chain Monte Carlo, Importance sampling, Count data, High frequency financial data",
abstract = "Simulation smoothing involves drawing state variables (or innovations) in discrete time state–space models from their conditional distribution given parameters and observations. Gaussian simulation smoothing is of particular interest, not only for the direct analysis of Gaussian linear models, but also for the indirect analysis of more general models. Several methods for Gaussian simulation smoothing exist, most of which are based on the Kalman filter. Since states in Gaussian linear state–space models are Gaussian Markov random fields, it is also possible to apply the Cholesky Factor Algorithm (CFA) to draw states. This algorithm takes advantage of the band diagonal structure of the Hessian matrix of the log density to make efficient draws. We show how to exploit the special structure of state–space models to draw latent states even more efficiently. We analyse the computational efficiency of Kalman-filter-based methods, the CFA, and our new method using counts of operations and computational experiments. We show that for many important cases, our method is most efficient. Gains are particularly large for cases where the dimension of observed variables is large or where one makes repeated draws of states for the same parameter values. We apply our method to a multivariate Poisson model with time-varying intensities, which we use to analyse financial market transaction count data."
}
@misc{OHara2017,
author = {O'Hara, Keith},
title = {{Bayesian Macroeconometrics in R}},
url = {http://www.kthohr.com/bmr.html},
urldate = {2017-10-17},
year = {2017}
}
@article{Primiceri2005,
    author = {Primiceri, Giorgio E.},
    title = {Time Varying Structural Vector Autoregressions and Monetary Policy},
    journal = {The Review of Economic Studies},
    volume = {72},
    number = {3},
    pages = {821-852},
    year = {2005},
    month = {07},
    abstract = "{Monetary policy and the private sector behaviour of the U.S. economy are modelled as a time varying structural vector autoregression, where the sources of time variation are both the coefficients and the variance covariance matrix of the innovations. The paper develops a new, simple modelling strategy for the law of motion of the variance covariance matrix and proposes an efficient Markov chain Monte Carlo algorithm for the model likelihood/posterior numerical evaluation. The main empirical conclusions are: (1) both systematic and non-systematic monetary policy have changed during the last 40 years—in particular, systematic responses of the interest rate to inflation and unemployment exhibit a trend toward a more aggressive behaviour, despite remarkable oscillations; (2) this has had a negligible effect on the rest of the economy. The role played by exogenous non-policy shocks seems more important than interest rate policy in explaining the high inflation and unemployment episodes in recent U.S. economic history.}",
    doi = {10.1111/j.1467-937X.2005.00353.x}
}
@article{Roberts2009,
author = {Gareth O. Roberts and Jeffrey S. Rosenthal},
title = {Examples of Adaptive {MCMC}},
journal = {Journal of Computational and Graphical Statistics},
volume = {18},
number = {2},
pages = {349--367},
year  = {2009},
publisher = {Taylor & Francis},
doi = {10.1198/jcgs.2009.06134}
}
@article{Rue2001,
author = {Rue, H{\aa}vard},
file = {:Users/sebastianankargren/Documents/Mendeley Desktop/Rue - 2001 - Fast sampling of Gaussian Markov random fields.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
keywords = {block sampling,conditional autoregressive model,divide-and-conquer strategy,eld,gaussian markov random,markov chain monte carlo,methods},
pages = {325--339},
title = {{Fast sampling of Gaussian Markov random fields}},
volume = {63},
year = {2001}
}
@article{Sanderson2016,
abstract = {The C++ language is often used for implementing functionality that is performance and/or resource sensitive. While the standard C++ library provides many useful algorithms (such as sorting), in its current form it does not provide direct handling of linear algebra (matrix maths). Armadillo is an open source linear algebra library for the C++ language, aiming towards a good balance between speed and ease of use. Its high-level Application Programming Interface (API) is deliberately similar to the widely Matlab and Octave languages (Eaton et al. 2015), so that mathematical operations can be expressed in a familiar and natural manner. The library is useful for algorithm development directly in C++, or relatively quick conversion of research code into production environments. Armadillo provides efficient objects for vectors, matrices and cubes (third order tensors), as well as over 200 associated functions for manipulating data stored in the objects. Integer, floating point and complex numbers are supported, as well as dense and sparse storage formats. Various matrix factorisations are provided through integration with LAPACK (Demmel 1997), or one of its high performance drop-in replacements such as Intel MKL (Intel 2016) or OpenBLAS (Xianyi, Qian, and Saar 2016). It is also possible to use Armadillo in conjunction with NVBLAS to obtain GPU-accelerated matrix multiplication (NVIDIA 2015). Armadillo is used as a base for other open source projects, such as MLPACK, a C++ library for machine learning and pattern recognition (Curtin et al. 2013), and RcppArmadillo, a bridge between the R language and C++ in order to speed up computations (Eddelbuettel and Sanderson 2014). Armadillo internally employs an expression evaluator based on template meta-programming techniques (Abrahams and Gurtovoy 2004), to automatically combine several operations in order to increase speed and efficiency. An overview of the internal architecture is given in (Sanderson 2010).},
author = {Sanderson, Conrad and Curtin, Ryan},
doi = {10.21105/joss.00026},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Sanderson, Curtin{\_}2016.pdf:pdf},
journal = {The Journal of Open Source Software},
number = {2},
title = {{Armadillo: A Template-Based C++ Library for Linear Algebra}},
volume = {1},
year = {2016}
}
@article{Schorfheide2015,
abstract = {This paper develops a vector autoregression (VAR) for macroeconomic time series which are observed at mixed frequencies – quarterly and monthly. The mixed-frequency VAR is cast in state-space form and estimated with Bayesian methods under a Minnesota-style prior. Using a real-time data set, we generate and evaluate forecasts from the mixed-frequency VAR and compare them to forecasts from a VAR that is estimated based on data time-aggregated to quarterly frequency. We document how information that becomes available within the quarter improves the forecasts in real time.},
author = {Schorfheide, Frank and Song, Dongho},
doi = {10.1080/07350015.2014.954707},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Schorfheide, Song{\_}2015.pdf:pdf},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {bayesian methods,macroeconomic forecasting,real-time data,vector autore-},
number = {3},
pages = {366--380},
title = {{Real-Time Forecasting with a Mixed-Frequency VAR}},
volume = {33},
year = {2015}
}
@article{Stock2001,
author = {Stock, James H and Watson, Mark W},
doi = {10.1257/jep.15.4.101},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Stock, Watson{\_}2001.pdf:pdf},
journal = {Journal of Economic Perspectives},
number = {4},
pages = {101--115},
title = {{Vector Autoregressions}},
volume = {15},
year = {2001}
}
  @Manual{R2019,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019},
    url = {https://www.R-project.org/},
  }
@article{Tusell2011,
author = {Tusell, Fernando},
doi = {10.18637/jss.v039.i02},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Tusell{\_}2011.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {chain linking,item response theory,mixed-format tests,r,separate calibration},
number = {2},
title = {{Kalman Filtering in R}},
volume = {39},
year = {2011}
}
@book{Wickham2009,
address = {New York},
author = {Wickham, Hadley},
doi = {10.1007/978-0-387-98141-3},
publisher = {Springer-Verlag},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
year = {2009}
}
@article{Wickham2014,
abstract = {In this paper we present the R package gRain for propagation in graphical indepen- dence networks (for which Bayesian networks is a special instance). The paper includes a description of the theory behind the computations. The main part of the paper is an illustration of how to use the package. The paper also illustrates how to turn a graphical model and data into an independence network.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0228},
author = {Wickham, Hadley},
doi = {10.18637/jss.v059.i10},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Wickham{\_}2014.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {data cleaning,data tidying,r,relational databases},
number = {10},
title = {{Tidy Data}},
volume = {59},
year = {2014}
}
@book{Wickham2015,
address = {Philadelphia, PA},
author = {Wickham, Hadley},
publisher = {Chapman and Hall/CRC},
title = {Advanced {R}},
year = {2015}
}
  @Manual{Wilke2018,
    title = {ggridges: Ridgeline Plots in 'ggplot2'},
    author = {Claus O. Wilke},
    year = {2018},
    note = {R package version 0.5.1},
    url = {https://CRAN.R-project.org/package=ggridges},
  }

@article{Villani2009,
author = {Villani, Mattias},
doi = {10.1002/jae.1065},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/Villani{\_}2009.pdf:pdf},
journal = {Journal of Applied Econometrics},
keywords = {bayesian inference,c11,c32,c53,cointegration,e50,forecasting,jel classification,unconditional mean,var,vecm},
number = {4},
pages = {630--650},
title = {{Steady State Priors for Vector Autoregressions}},
volume = {24},
year = {2009}
}
@article{Osterholm2010,
abstract = {The theory that we shall seek to elaborate here puts considerable emphasis on the impact of cross-border capital flows on domestic financial conditions, the internal financing power of corporate growth, the limits of outside financing through equity capital, the internal financing power of corporate growth, and the convergence of financial systems. The paper generates insights about the emergence of financial conglomerates, the challenges raised by the integration of the financial markets, the global evolution of the economic and financial activity, the European market of financial services, and the creation of the single market for financial services.},
author = {{\"{O}}sterholm, P{\"{a}}r},
doi = {10.1080/09603100903357408},
file = {:C$\backslash$:/Users/seban876/Documents/Mendeley Desktop/{\"{O}}sterholm{\_}2010.pdf:pdf},
journal = {Applied Financial Economics},
number = {4},
pages = {265--274},
title = {{The Effect on the Swedish Real Economy of the Financial Crisis}},
volume = {20},
year = {2010}
}


